# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pf66G2t_4vXZuFzuIgG1hcSAtny6DUzH

# **PIX TO PIX GAN TO GENERATE RIGHT SIDE OF ANAGLYPHIC IMAGE**

I want to build a gan to generate right image of a anaglyphic image when given left as input 

I have prepared my dataset with training and validation directory 
with each of those folders having 2 subfolders one subfolder containing left images and another containing right images

##want to build a GAN using TensorFlow 
that will take a left image from an anaglyphic image pair as input, and generate the corresponding right image.

### Here are the general steps you can follow to build your GAN:

**Load the dataset:** Load the left and right images from your prepared dataset into TensorFlow.

**Preprocess the data:** Resize the images to a suitable size and normalize the pixel values to be between -1 and 1.

**Define the generator:** Define the generator network using TensorFlow. This network should take in a left image and generate the corresponding right image.

**Define the discriminator:** Define the discriminator network using TensorFlow. This network should take in an image (either left or generated right) and classify it as real or fake.

**Define the loss functions:** Define the loss functions for both the generator and discriminator networks. The generator loss function should encourage the generator to produce realistic-looking right images. The discriminator loss function should encourage the discriminator to correctly classify real and fake images.

**Train the GAN:** Train the GAN by alternating between training the discriminator and generator networks. For each training step, you will feed a batch of left images to the generator to generate fake right images. You will then train the discriminator using a combination of real and fake right images.

**Evaluate the GAN:** Evaluate the performance of the GAN by testing it on a separate validation dataset.
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

"""# Load the dataset:"""

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import os
import time
import matplotlib.pyplot as plt
from IPython import display

train_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/Users/abhiramkamini/Downloads/paper/dataset/train/",
    label_mode=None,
    image_size=(724, 640),
    batch_size=32,
)

val_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/Users/abhiramkamini/Downloads/paper/dataset/valid/",
    label_mode=None,
    image_size=(724, 640),
    batch_size=32,
)

"""# Preprocess the data:"""

def preprocess(image):
    image = tf.image.resize(image, (724, 640))
    image = tf.cast(image, tf.float32)
    image = (image / 127.5) - 1
    return image

train_data = train_data.map(preprocess)
val_data = val_data.map(preprocess)

"""# Define the generator:"""

def generator_model():
    inputs = tf.keras.layers.Input(shape=(724, 640, 3))
    x = tf.keras.layers.Conv2D(64, 3, strides=2, padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU()(x)
    
    # Add more convolutional layers here
    
    x = tf.keras.layers.Conv2DTranspose(3, 3, strides=2, padding="same", activation="tanh")(x)
    
    model = tf.keras.models.Model(inputs=inputs, outputs=x)
    return model

generator = generator_model()
generator.summary()

"""# Define the discriminator:"""

def discriminator_model():
    inputs = tf.keras.layers.Input(shape=(724, 640, 3))
    x = tf.keras.layers.Conv2D(64, 3, strides=2, padding="same")(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU()(x)
    
    # Add more convolutional layers here
    
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(1)(x)
    
    model = tf.keras.models.Model(inputs=inputs, outputs=x)
    return model

discriminator = discriminator_model()
discriminator.summary()

"""# Define the loss functions:"""

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

"""# Train the GAN:"""

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

@tf.function
def train_step(images):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(images, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

EPOCHS = 100
for epoch in range(EPOCHS):
    print(epoch)
    for images in train_data:
        gen_loss, disc_loss = train_step(images)

    print(f"Epoch {epoch + 1}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}")

generator.save("gen.h5")
"""# generate images using the trained generator"""

import numpy as np
import matplotlib.pyplot as plt

# Generate anaglyphic images using trained generator
def generate_images(model, test_input):
    predictions = model(test_input, training=False)
    return predictions

# Select a random image from the test dataset
test_image = next(iter(test_data))

# Generate anaglyphic image using the trained generator
generated_image = generate_images(generator, test_image)

# Plot the original left image, the ground truth right image, and the generated right image
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
axs[0].imshow(test_image[0, :, :, :])
axs[0].set_title("Left Image")
axs[1].imshow(test_image[1, :, :, :])
axs[1].set_title("Ground Truth Right Image")
axs[2].imshow(generated_image[0, :, :, :])
axs[2].set_title("Generated Right Image")
plt.show()

#alternate discriminator

# Define discriminator with more convolutional layers
def make_discriminator_model():
    model = tf.keras.Sequential()

    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[724, 640, 3]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

#alternate generator 
# Define generator with more convolutional layers
def make_generator_model():
    model = tf.keras.Sequential()

    model.add(layers.Dense(91*80*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((91, 80, 256)))
    assert model.output_shape == (None, 91, 80, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 91, 80, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 182, 160, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 364, 320, 32)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 724, 640, 16)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 724, 640, 3)

    return model